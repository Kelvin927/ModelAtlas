{
  "id": "linear-regression",
  "name": "Linear Regression",
  "aliases": [],
  "category": "Regression",
  "tags": [
    "regression",
    "ols",
    "supervised"
  ],
  "level": "beginner",
  "summary": "Fits a linear mapping from features to a continuous target using ordinary least squares; a fast, interpretable baseline.",
  "app_scenarios": [
    "House price prediction",
    "Sales forecasting",
    "Exam score prediction"
  ],
  "math": {
    "core_formula": "\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y, \\quad \\hat{y} = X\\hat{\\beta}",
    "derivation": [
      "J(\\beta)=\\|y-X\\beta\\|_2^2",
      "\\nabla_\\beta J=-2X^\\top(y-X\\beta)=0",
      "\\Rightarrow X^\\top X\\hat\\beta=X^\\top y",
      "\\hat\\beta=(X^\\top X)^{-1}X^\\top y"
    ],
    "assumptions": [
      "Linearity of expectation E[y|X]=Xβ",
      "Independence of errors",
      "Homoskedasticity Var(ε|X)=σ²I",
      "No perfect multicollinearity",
      "Normal errors for inference"
    ],
    "constraints": [],
    "variants": [
      "Multiple Linear Regression",
      "Ridge",
      "Lasso",
      "Elastic Net"
    ]
  },
  "hyperparameters": [],
  "data_requirements": {
    "input": "Continuous target; Approximate linear relation; Low multicollinearity",
    "scale_sensitivity": "",
    "missing_values": ""
  },
  "required_columns": [
    {"role":"feature_x","canonical":"x","aliases":["X","feat_x"],"dtype":"number","description":"Independent variable"},
    {"role":"target_y","canonical":"y","aliases":["Y","target"],"dtype":"number","description":"Dependent variable"}
  ],
  "code": {
    "python_sklearn": "import pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.read_csv('data.csv')\nX = df[['x']].values; y = df['y'].values\nmodel = LinearRegression().fit(X,y)\npred = model.predict(X)\n\nplt.scatter(X[:,0], y)\nplt.plot(X[:,0], pred, color='red')\nplt.savefig('output.png')\n\nprint('coef=', model.coef_, 'intercept=', model.intercept_)\npd.DataFrame({'x':X[:,0],'y':y,'y_hat':pred}).to_csv('result.csv', index=False)"
  },
  "usage": {
    "workflow": [
      "Explore data; check linearity/outliers",
      "Split train/test; optionally K-fold CV",
      "Encode categoricals; standardize if using regularization",
      "Fit OLS; examine coefficients",
      "Diagnostics: residual vs fitted, QQ-plot, VIF"
    ],
    "examples": []
  },
  "evaluation": {
    "metrics": [
      "MSE",
      "RMSE",
      "R²"
    ],
    "validation": "5-fold cross-validation with shuffle; use stratified bins if target heavily skewed",
    "baselines": [
      "Mean predictor (always predict ȳ)"
    ]
  },
  "strengths": [
    "Simple and interpretable",
    "Fast to train",
    "Provides coefficient insights"
  ],
  "weaknesses": [
    "Assumes linearity",
    "Sensitive to outliers",
    "Multicollinearity issues"
  ],
  "pitfalls": [
    "Using R² alone on non-comparable datasets",
    "Ignoring correlated errors/time series structure",
    "Extrapolating far outside training range"
  ],
  "comparisons": [
    {
      "with": "Ridge",
      "differences": [
        "Ridge trades variance for bias via L2; better with multicollinearity"
      ]
    },
    {
      "with": "Lasso",
      "differences": [
        "Lasso performs feature selection via sparsity; OLS does not"
      ]
    }
  ],
  "visualizations": [
    "Scatter plot with regression line",
    "residual plot",
    "Residuals vs fitted",
    "QQ-plot of residuals",
    "Coefficient bar chart"
  ],
  "references": [
    {
      "title": "https://en.wikipedia.org/wiki/Linear_regression",
      "url": "https://en.wikipedia.org/wiki/Linear_regression",
      "type": "docs"
    },
    {
      "title": "https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares",
      "url": "https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares",
      "type": "docs"
    }
  ],
  "related": [
    "scikit-learn",
    "statsmodels",
    "numpy",
    "pandas"
  ],
  "last_updated": "2025-08-24"
}