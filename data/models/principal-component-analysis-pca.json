{
  "id": "principal-component-analysis-pca",
  "name": "Principal Component Analysis (PCA)",
  "aliases": [],
  "category": "Dimensionality Reduction",
  "tags": [
    "pca",
    "unsupervised",
    "projection"
  ],
  "level": "beginner",
  "summary": "Projects data to orthogonal axes of maximal variance; useful for compression, noise reduction and visualization.",
  "app_scenarios": [
    "Noise reduction",
    "Visualization",
    "Preprocessing"
  ],
  "math": {
    "core_formula": "W = \\text{eigenvectors}(\\Sigma),\\; Z = XW",
    "derivation": [
      "\\Sigma=\\frac{1}{n}X^\\top X",
      "Eigen-decompose \\Sigma=Q\\Lambda Q^\\top",
      "Take top-k eigenvectors as principal components"
    ],
    "assumptions": [
      "Linear structure captures variance",
      "Mean-centered data"
    ],
    "constraints": [],
    "variants": [
      "Kernel PCA",
      "Sparse PCA",
      "Incremental PCA"
    ]
  },
  "hyperparameters": [],
  "data_requirements": {
    "input": "Centering data; Optionally scaling",
    "scale_sensitivity": "",
    "missing_values": ""
  },
  "required_columns": [
    {"role":"feature_x","canonical":"x","aliases":["X"],"dtype":"number","description":"Feature"},
    {"role":"feature_y","canonical":"y","aliases":["Y"],"dtype":"number","description":"Feature"}
  ],
  "code": {
    "python_sklearn": "import pandas as pd\nfrom sklearn.decomposition import PCA\n\ndf = pd.read_csv('data.csv')\nX = df[['x','y']]\nmodel = PCA(n_components=2).fit(X)\n\nprint('explained variance', model.explained_variance_ratio_)\npd.DataFrame(model.transform(X),columns=['pc1','pc2']).to_csv('result.csv',index=False)"
  },
  "usage": {
    "workflow": [
      "Center (and often scale) features",
      "Decide k via explained variance",
      "Inspect loadings for interpretation"
    ],
    "examples": []
  },
  "evaluation": {
    "metrics": [
      "Explained variance ratio",
      "Reconstruction error"
    ],
    "validation": "Report cumulative explained variance",
    "baselines": [
      "Random projections for comparison"
    ]
  },
  "strengths": [
    "Reduces dimensionality",
    "Uncorrelated components"
  ],
  "weaknesses": [
    "Components hard to interpret",
    "Linear method"
  ],
  "pitfalls": [
    "Components hard to interpret",
    "Sensitive to scaling and outliers"
  ],
  "comparisons": [
    {
      "with": "Kernel PCA",
      "differences": [
        "Kernel PCA captures nonlinear manifolds"
      ]
    }
  ],
  "visualizations": [
    "Scree plot",
    "biplot",
    "2D projection",
    "Biplot",
    "2D/3D component scatter"
  ],
  "references": [
    {
      "title": "https://en.wikipedia.org/wiki/Principal_component_analysis",
      "url": "https://en.wikipedia.org/wiki/Principal_component_analysis",
      "type": "docs"
    },
    {
      "title": "https://scikit-learn.org/stable/modules/decomposition.html#pca",
      "url": "https://scikit-learn.org/stable/modules/decomposition.html#pca",
      "type": "docs"
    }
  ],
  "related": [
    "scikit-learn",
    "numpy"
  ],
  "last_updated": "2025-08-24"
}