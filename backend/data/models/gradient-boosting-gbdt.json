{
  "id": "gradient-boosting-gbdt",
  "name": "Gradient Boosting (GBDT)",
  "aliases": [],
  "category": "Ensemble",
  "tags": [
    "boosting",
    "ensemble"
  ],
  "level": "beginner",
  "summary": "Sequentially fits weak learners to residuals; flexible and accurate but requires careful tuning.",
  "app_scenarios": [
    "Tabular regression",
    "Ranking"
  ],
  "math": {
    "core_formula": "F_m(x) = F_{m-1}(x) + \\nu h_m(x), \\; h_m = \\arg\\min_h \\sum L(y_i, F_{m-1}(x_i) + h(x_i))",
    "derivation": [
      "Initialize F_0(x)=\\arg\\min_\\gamma \\sum L(y_i,\\gamma)",
      "For m=1..M: fit h_m to negative gradients",
      "Update F_m=F_{m-1}+\\nu \\rho h_m"
    ],
    "assumptions": [
      "Additive models with small learning rate improve generalization"
    ],
    "constraints": [],
    "variants": [
      "XGBoost",
      "LightGBM",
      "CatBoost"
    ]
  },
  "hyperparameters": [
    {
      "name": "n_estimators",
      "type": "int",
      "default": 100,
      "tips": "Increase with smaller learning rate"
    },
    {
      "name": "learning_rate",
      "type": "float",
      "default": 0.1,
      "tips": "Lower values need more trees"
    },
    {
      "name": "max_depth",
      "type": "int",
      "default": 3,
      "tips": "Shallow trees as weak learners"
    }
  ],
  "data_requirements": {
    "input": "Feature scaling optional; Careful tuning of learning rate/trees",
    "scale_sensitivity": "",
    "missing_values": ""
  },
  "required_columns": [
    {"role":"feature_x","canonical":"x","aliases":["X"],"dtype":"number","description":"Feature"},
    {"role":"feature_y","canonical":"y","aliases":["Y"],"dtype":"number","description":"Feature"},
    {"role":"label","canonical":"label","aliases":["target"],"dtype":"number","description":"Class label"}
  ],
  "code": {
    "python_sklearn": "import pandas as pd\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ndf = pd.read_csv('data.csv')\nX = df[['x','y']]; y = df['label']\nmodel = GradientBoostingClassifier().fit(X,y)\nprint('score=', model.score(X,y))\npd.DataFrame({'pred':model.predict(X)}).to_csv('result.csv', index=False)"
  },
  "usage": {
    "workflow": [
      "Scale optional",
      "Tune learning_rate vs n_estimators tradeoff",
      "Early stopping with validation set",
      "Use subsample<1.0 for stochastic boosting"
    ],
    "examples": []
  },
  "evaluation": {
    "metrics": [
      "MSE/RMSE",
      "MAE",
      "RÂ²"
    ],
    "validation": "Train/validation split with early stopping",
    "baselines": [
      "Random Forest"
    ]
  },
  "strengths": [
    "High accuracy",
    "Handles nonlinearity"
  ],
  "weaknesses": [
    "Sensitive to hyperparameters",
    "Longer training time"
  ],
  "pitfalls": [
    "Too large learning rate causes overfit",
    "Insensitive to monotonic constraints unless supported"
  ],
  "comparisons": [
    {
      "with": "XGBoost",
      "differences": [
        "XGBoost adds regularization, column/block structure, and advanced split finding"
      ]
    }
  ],
  "visualizations": [
    "Feature importance",
    "SHAP values",
    "Training/validation loss curve"
  ],
  "references": [
    {
      "title": "https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting",
      "url": "https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting",
      "type": "docs"
    }
  ],
  "related": [
    "scikit-learn"
  ],
  "last_updated": "2025-08-24"
}