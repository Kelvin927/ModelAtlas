{
  "id": "xgboost",
  "name": "XGBoost",
  "aliases": [],
  "category": "Ensemble",
  "tags": [
    "xgboost",
    "boosting",
    "ensemble"
  ],
  "level": "beginner",
  "summary": "Highly-optimized gradient boosting with regularization and efficient split finding; strong on tabular benchmarks.",
  "app_scenarios": [
    "Kaggle/tabular benchmarks",
    "Credit scoring",
    "CTR prediction"
  ],
  "math": {
    "core_formula": "Obj = \\sum l(y_i, \\hat{y}_i) + \\sum \\Omega(f_k), \\; \\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\lVert w \\rVert^2",
    "derivation": [],
    "assumptions": [],
    "constraints": [],
    "variants": [
      "XGBoost GPU",
      "DART"
    ]
  },
  "hyperparameters": [
    {
      "name": "n_estimators",
      "type": "int",
      "default": 300,
      "tips": "Use early stopping with eval_set"
    },
    {
      "name": "max_depth",
      "type": "int",
      "default": 6,
      "tips": "Shallower trees generalize better"
    },
    {
      "name": "learning_rate",
      "type": "float",
      "default": 0.1,
      "tips": "Lower for better generalization"
    },
    {
      "name": "subsample",
      "type": "float",
      "default": 1.0,
      "tips": "0.6â€“0.9 often helps"
    }
  ],
  "data_requirements": {
    "input": "Careful tuning (eta, max_depth, subsample); Categoricals need encoding",
    "scale_sensitivity": "",
    "missing_values": ""
  },
  "required_columns": [
    {
      "role": "feature_x",
      "canonical": "x",
      "aliases": ["X"],
      "dtype": "number",
      "description": "Feature"
    },
    {
      "role": "feature_y",
      "canonical": "y",
      "aliases": ["Y"],
      "dtype": "number",
      "description": "Feature"
    },
    {
      "role": "label",
      "canonical": "label",
      "aliases": ["target"],
      "dtype": "number",
      "description": "Class label"
    }
  ],
  "code": {
    "python_sklearn": "import pandas as pd\nimport xgboost as xgb\n\n# Read data\ndf = pd.read_csv('data.csv')\nX = df[['x','y']]; y = df['label']\n\nmodel = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss').fit(X,y)\n\nprint('score=', model.score(X,y))\npd.DataFrame({'pred':model.predict(X)}).to_csv('result.csv', index=False)"
  },
  "usage": {
    "workflow": [
      "Encode categoricals (unless using CatBoost elsewhere)",
      "Set eval_set and early_stopping_rounds",
      "Tune depth/eta/subsample/colsample",
      "Check feature importance with care"
    ],
    "examples": []
  },
  "evaluation": {
    "metrics": [
      "Accuracy/ROC-AUC (classification)",
      "RMSE (regression)"
    ],
    "validation": "Stratified CV or early-stopping on validation",
    "baselines": [
      "GBDT",
      "Random Forest"
    ]
  },
  "strengths": [
    "Strong performance",
    "Regularized",
    "Handles missing values"
  ],
  "weaknesses": [
    "Many hyperparameters",
    "Potential overfitting if untuned"
  ],
  "pitfalls": [
    "Data leakage via CV",
    "Overfitting with too many trees or high depth"
  ],
  "comparisons": [
    {
      "with": "LightGBM",
      "differences": [
        "LightGBM uses histogram + leaf-wise growth; often faster on large data"
      ]
    }
  ],
  "visualizations": [
    "Feature importance",
    "SHAP plots"
  ],
  "references": [
    {
      "title": "https://xgboost.readthedocs.io/en/stable/",
      "url": "https://xgboost.readthedocs.io/en/stable/",
      "type": "docs"
    }
  ],
  "related": [
    "xgboost",
    "numpy",
    "pandas"
  ],
  "last_updated": "2025-08-24"
}